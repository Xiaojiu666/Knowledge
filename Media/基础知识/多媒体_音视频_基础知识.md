## 图像知识

#### 表示形式和大小:

任何一个图像都可以由RGB组成，常用的表示方式有以下几种。 

- 浮点表示：取值范围为0.0～1.0，比如，在OpenGL ES中对每一个子 像素点的表示使用的就是这种表达方式。 

- 整数表示：取值范围为0～255或者00～FF，8个比特表示一个子像 素，32个比特表示一个像素，这就是类似于某些平台上表示图像格式的 RGBA_8888数据格式。比如，Android平台上RGB_565的表示方法为16比 特模式表示一个像素，R用5个比特来表示，G用6个比特来表示，B用5个 比特来表示。 

对于一幅图像，一般使用整数表示方法来进行描述，比如计算一张 1280×720的RGBA_8888图像的大小，可采用如下方式：

​			1280 * 720 * 4 = 3.516MB 



#### 图像格式:

###### YUV：

###### RGB：



##	压缩知识

#### 图像压缩算法：

###### JPEG图像压缩：

上面计算方式是位图（bitmap）在内存中所占用的大小，所以每一张图像的裸数据都是很大的。对于图像的裸数据来讲，直接在网络上进行传输也是 不太可能的，所以就有了图像的压缩格式，比如JPEG压缩：JPEG是静态 图像压缩标准，由ISO制定。JPEG图像压缩算法在提供良好的压缩性能 的同时，具有较好的重建质量。这种算法被广泛应用于图像处理领域，当 然其也是一种有损压缩。

###### MPEG视频压缩:



#### 视频压缩算法

视频压缩算法，在编/解码过程中，将视频中每一帧通过算法进行压缩： YUV原始体量太大，无论对网络还是硬盘要求过大，

###### MPEG:

MPEG（1/2/3/4）系列由MPEG（Moving Picture Experts Group, ISO旗下的组织）主导。

对于视频，ISO同样也 制定了标准：Motion JPEG即MPEG，MPEG算法是适用于动态视频的压缩 算法，它除了对单幅图像进行编码外，还利用图像序列中的相关原则去 除冗余，这样可以大大提高视频的压缩比。截至目前，MPEG的版本一直 在不断更新中，主要包括这样几个版本：Mpeg1（用于VCD）、Mpeg2（用于 DVD）、Mpeg4 AVC（现在流媒体使用最多的就是它了）。 

###### H264:

联合制定的编码标准，那就是现在主流的编码格式H264，当然还有下一代更先进的压缩编码标准H265。

H.264创造了多参考帧、多块类型、整数变换、帧内预测等新的压缩技术，使用了更精细的分像素运动矢量（1/4、1/8）和新一代的环路滤波器， 

这使得压缩性能得到大大提高，系统也变得更加完善。



#### 编码概念

###### 视频帧

视频是由一帧一帧画面构成的，但是在视频的数据中，并不是真正按照一帧一帧原始数据保存下来的（如果这样，压缩编码就没有意义了）。

视频压缩中，每帧都代表着一幅静止的图像。而在进行实际压缩时， 会采取各种算法以减少数据的容量，其中IPB帧就是最常见的一种。

H264会根据一段时间内，画面的变化情况，选取一帧画面作为完整编码，下一帧只记录与上一帧完整数据的差别，是一个动态压缩的过程。

在H264中，三种类型的帧数据分别为:

- **I帧**：帧内编码帧（intra picture），也称关键帧，I帧通常是每个GOP（MPEG所使用的一种视频压缩技术）的第一个帧，经过适度地压缩，作为随机访问的参 考点，可以当成静态图像。I帧可以看作一个图像经过压缩后的产物，I帧压缩可以得到6：1的压缩比而不会产生任何可觉察的模糊现象。I帧压缩可去掉视频的空间冗余信息，下面即将介绍的P帧和B帧是为了去掉时间冗余信息。(空间维度)，你可以理解为这一帧画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）

- **P帧**：帧间编码，前向预测编码帧。前向预测编码帧（predictive-frame），通过将图像序列中前面已 编码帧的时间冗余信息充分去除来压缩传输数据量的编码图像，也称为 预测帧。 (时间维度)，P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧的画面差别的数据）

- **B帧**：帧间编码，双向预测内插编码帧（bi-directional interpolated predictionframe），既考虑源图像序列前面的已编码帧，又顾及源图像序列后面的已 编码帧之间的时间冗余信息，来压缩传输数据量的编码图像，也称为双向预测帧。 (时间维度)B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别（具体比较复杂，有4种情况，但我这样说简单些，有兴趣可以看看我上面提供的资料），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码时CPU会比较累~。

- **IDR帧：**IDR都是I帧，可以防止一帧解码出错，导致后面所有帧解码出错的问题。当解码器在解码到IDR的时候，会将之前的参考帧清空，重新开始一个新的序列，这样，即便前面一帧解码出现重大错误，也不会蔓延到后面的数据中。eg：在网络相机传输的过程中， 只用到了IDR帧和P帧，原因是如果I帧间隔过长，网络传输丢包时，会造成严重的黑屏/花屏等现象，使用IDR帧 会在下一个gop来临之前清除上帧内容。

  ![image-20210330185615818](/Users/edz/Library/Application Support/typora-user-images/image-20210330185615818.png)

  由于B帧双向参考，所以解码时和显示顺序会有出入，B帧会向前和后参考，如果后向没有压缩好图片，则等待，但是显示顺序不可能错乱(1.3.4.2)，所以会进行缓存，等待解码完成后显示顺序

###### 图像组

全称：Group of picture。指一组变化不大的视频帧。

GOP的第一帧成为关键帧：IDR

IDR都是I帧，可以防止一帧解码出错，导致后面所有帧解码出错的问题。当解码器在解码到IDR的时候，会将之前的参考帧清空，重新开始一个新的序列，这样，即便前面一帧解码出现重大错误，也不会蔓延到后面的数据中。

> 注：关键帧都是I帧，但是I帧不一定是关键帧



###### **DTS与PTS**

DTS全称：Decoding Time Stamp。标示读入内存中数据流在什么时候开始送入解码器中进行解码。也就是解码顺序的时间戳。

PTS全称：Presentation Time Stamp。解码阶段进行视频的同步和输出用于标示解码后的视频帧什么时候被显示出来。

> 在没有B帧的情况下，DTS和PTS的输出顺序是一样的，一旦存在B帧，PTS和DTS则会不同。





关于Anroid播放器的编解码，主要分为硬编码和软编码。

| 软编码 | CPU        | 通过FFmpeg     | 具有更好的适应性，但是占用了更多的CUP那就意味着很耗费性能，很耗电， |
| ------ | ---------- | -------------- | ------------------------------------------------------------ |
| 硬编码 | GPU/解码器 | 通过MediaCodec | 每个设备支持不同 |



##	视频播放知识

<img src="/Users/edz/Library/Application Support/typora-user-images/image-20210421172309615.png" alt="image-20210421172309615" style="zoom: 25%;" />



播放一个互联网上的视频文件，需要经过以下几个步骤：解协议，解封装，解码视音频，视音频同步。

播放本地文件则不需要解协议，为以下几个步骤：解封装，解码视音频，视音频同步。他们的过程如图所示。



####	解协议

将流媒体协议的数据，解析为标准的相应的封装格式数据。视音频在网络上传播的时候，常常采用各种流媒体协议，例如HTTP，RTMP，或是MMS等等。这些协议在传输视音频数据的同时，也会传输一些信令数据。这些信令数据包括对播放的控制（播放，暂停，停止），或者对网络状态的描述等。解协议的过程中会去除掉信令数据而只保留视音频数据。例如，采用RTMP协议传输的数据，经过解协议操作后，输出FLV格式的数据。



####	解封装

将输入的封装格式的数据，分离成为音频流压缩编码数据和视频流压缩编码数据。封装格式种类很多，例如MP4，MKV，RMVB，TS，FLV，AVI等等，它的作用就是将已经压缩编码的视频数据和音频数据按照一定的格式放到一起。例如，FLV格式的数据，经过解封装操作后，输出H.264编码的视频码流和AAC编码的音频码流。



#### 解码

将视频/音频压缩编码数据，解码成为非压缩的视频/音频原始数据。音频的压缩编码标准包含AAC，MP3，AC-3等等，视频的压缩编码标准则包含H.264，MPEG2，VC-1等等。解码是整个系统中最重要也是最复杂的一个环节。通过解码，压缩编码的视频数据输出成为非压缩的颜色数据，例如YUV420P，RGB等等；压缩编码的音频数据输出成为非压缩的音频抽样数据，例如PCM数据。



####	音视频同步

根据解封装模块处理过程中获取到的参数信息，同步解码出来的视频和音频数据，并将视频音频数据送至系统的显卡和声卡播放出来。



##	视频传输协议

####	RTMP

由Adobe公司提出的，在互联网TCP/IP五层体系结构中应用层，RTMP协议是基于TCP协议的，也就是说RTMP实际上是使用TCP作为传输协议。TCP协议在处在传输层，是面向连接的协议，能够为数据的传输提供可靠保障，因此数据在网络上传输不会出现丢包的情况。不过这种可靠的保障也会造成一些问题，也就是说前面的数据包没有交付到目的地，后面的数据也无法进行传输。幸运的是，目前的网络带宽基本上可以满足RTMP协议传输普通质量视频的要求。

RTMP传输的数据的基本单元为Message，但是实际上传输的最小单元是Chunk（消息块），因为RTMP协议为了提升传输速度，在传输数据的时候，会把Message拆分开来，形成更小的块，这些块就是Chunk。

######	消息（Message）的结构

![img](https://upload-images.jianshu.io/upload_images/7114429-e3c87713a95cb199.png?imageMogr2/auto-orient/strip|imageView2/2/w/777)

Message结构分析

- Message Type：它是一个消息类型的ID，通过该ID接收方可以判断接收到的数据的类型，从而做相应的处理。Message Type ID在1-7的消息用于协议控制，这些消息一般是RTMP协议自身管理要使用的消息，用户一般情况下无需操作其中的数据。Message Type ID为8，9的消息分别用于传输音频和视频数据。Message Type ID为15-20的消息用于发送AMF编码的命令，负责用户与服务器之间的交互，比如播放，暂停等。

- Playload Length： 消息负载的长度，即音视频相关信息的的数据长度，4个字节
- TimeStamp：时间戳，3个字节。

- Stream ID：消息的唯一标识。拆分消息成Chunk时添加该ID，从而在还原时根据该ID识别Chunk属于哪个消息。

- Message Body：消息体，承载了音视频等信息。



###### 消息块（Chunk）

![img](https://upload-images.jianshu.io/upload_images/7114429-77afde36a4ff40dd.png?imageMogr2/auto-orient/strip|imageView2/2/w/701)

通过上图可以看出，消息块在结构上与与消息类似，有Header和Body。

- Basic Header：基本的头部信息，在头部信息里面包含了chunk stream ID（流通道Id，用来标识指定的通道）和chunk type（chunk的类型）。

- Message Header：消息的头部信息，包含了要发送的实际信息（可能是完整的，也可能是一部分）的描述信息。Message Header的格式和长度取决于Basic Header的chunk type。

- Extended TimeStamp：扩展时间戳。

- Chunk Data：块数据。

RTMP在传输数据的时候，发送端会把需要传输的媒体数据封装成消息，然后把消息拆分成消息块，再一个一个进行传输。接收端收到消息块后，根据Message Stream ID重新将消息块进行组装、组合成消息，再解除该消息的封装处理就可以还原出媒体数据。由此可以看出，RTMP收发数据是以Chunk为单位，而不是以Message为单位。需要注意的是，RTMP发送Chunk必须是一个一个发送，后面的Chunk必须等前面的Chunk发送完成。



####	RTSP

RTSP（Real Time Streaming Protocol）是TCP/UDP协议体系中的一个应用层协议，由哥伦比亚大学, 网景和RealNetworks公司提交的IETF RFC标准.该协议定义了一对多应用程序如何有效地通过IP网络传输多媒体数据。RTSP在体系结构上位于RTP和RTCP之上，它使用TCP或者RTP完成数据传输，目前市场上大多数采用RTP来传输媒体数据。

RTSP和RTP/RTCP之间是什么关系呢？下面是一个经典的流媒体传输流程图

![img](https://upload-images.jianshu.io/upload_images/7114429-4ca0e3407de8580a.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/440)



一次基本的RTSP操作过程:

首先，客户端连接到流服务器并发送一个RTSP描述命令（DESCRIBE）。

流服务器通过一个SDP描述来进行反馈，反馈信息包括流数量、媒体类型等信息。

客户端再分析该SDP描述，并为会话中的每一个流发送一个RTSP建立命令(SETUP)，RTSP建立命令告诉服务器客户端用于接收媒体数据的端口。流媒体连接建立完成后，

客户端发送一个播放命令(PLAY)，服务器就开始在UDP上传送媒体流（RTP包）到客户端。 在播放过程中客户端还可以向服务器发送命令来控制快进、快退和暂停等。

最后，客户端可发送一个终止命令(TERADOWN)来结束流媒体会话。

由上图可以看出，RTSP处于应用层，而RTP/RTCP处于传输层。RTSP负责建立以及控制会话，RTP负责多媒体数据的传输。而RTCP是一个实时传输控制协议，配合RTP做控制和流量监控。封装发送端及接收端（主要）的统计报表。这些信息包括丢包率，接收抖动等信息。发送端根据接收端的反馈信息做响应的处理。RTP与RTCP相结合虽然保证了实时数据的传输，但也有自己的缺点。最显著的是当有许多用户一起加入会话进程的时候，由于每个参与者都周期发送RTCP信息包，导致RTCP包泛滥(flooding)。

RTSP的请求报文结构如下图

<img src="https://upload-images.jianshu.io/upload_images/7114429-a555d2f1f2992323.png?imageMogr2/auto-orient/strip|imageView2/2/w/521" alt="img" style="zoom:67%;" />

简单的RTSP消息交互过程

C表示RTSP客户端,S表示RTSP服务端

第一步：查询服务器端可用方法

C->S OPTION request //询问S有哪些方法可用

S->C OPTION response //S回应信息的public头字段中包括提供的所有可用方法

第二步：得到媒体描述信息

C->S DESCRIBE request //要求得到S提供的媒体描述信息

S->C DESCRIBE response //S回应媒体描述信息，一般是sdp信息

第三步：建立RTSP会话

C->S SETUP request //通过Transport头字段列出可接受的传输选项，请求S建立会话

S->C SETUP response //S建立会话，通过Transport头字段返回选择的具体转输选项，并返回建立的Session ID;

第四步：请求开始传送数据

C->S PLAY request //C请求S开始发送数据

S->C PLAY response //S回应该请求的信息

第五步： 数据传送播放中

S->C 发送流媒体数据 // 通过RTP协议传送数据

第六步：关闭会话，退出

C->S EARDOWN request //C请求关闭会话

S->C TEARDOWN response //S回应该请求

上述的过程只是标准的、友好的rtsp流程，但实际的需求中并不一定按此过程。 其中第三和第四步是必需的！第一步，只要服务器和客户端约定好有哪些方法可用，则option请求可以不要。第二步，如果我们有其他途径得到媒体初始化描述信息（比如http请求等等），则我们也不需要通过rtsp中的describe请求来完成。



####	HLS

**HTTP Live Streaming**（缩写是**HLS**）是一个由苹果公司提出的基于Http协议的的流媒体网络传输协议。是苹果公司[QuickTime X](https://link.jianshu.com/?t=https://zh.wikipedia.org/w/index.php?title=QuickTime_X&action=edit&redlink=1)和[iPhone](https://link.jianshu.com/?t=https://zh.wikipedia.org/wiki/IPhone)软件系统的一部分。它的工作原理是把整个流分成一个个小的基于HTTP的文件来下载，每次只下载一些。当媒体流正在播放时，客户端可以选择从许多不同的备用源中以不同的速率下载同样的资源，允许流媒体会话适应不同的数据速率。在开始一个流媒体会话时，客户端会下载一个包含元数据的[extended M3U (m3u8)](https://link.jianshu.com/?t=https://zh.wikipedia.org/w/index.php?title=Extended_M3U&action=edit&redlink=1)[playlist](https://link.jianshu.com/?t=https://zh.wikipedia.org/w/index.php?title=Playlist&action=edit&redlink=1)文件，用于寻找可用的媒体流。

HLS协议的优点：

1.跨平台性：支持iOS/Android/浏览器，通用性强。

2.穿墙能力强：由于HLS是基于HTTP协议的，因此HTTP数据能够穿透的防火墙或者代理服务器HLS都可以做到，基本不会遇到被防火墙屏蔽的情况。

3.切换码率快（清晰度）：自带多码率自适应，客户端可以选择从许多不同的备用源中以不同的速率下载同样的资源，允许流媒体会话适应不同的数据速率。客户端可以很快的选择和切换码率，以适应不同带宽条件下的播放。

3.负载均衡：HLS基于无状态协议（HTTP），客户端只是按照顺序使用下载存储在服务器的普通TS文件，做负责均衡如同普通的HTTP文件服务器的负载均衡一样简单。

HLS的缺点：

1.实时性差：苹果官方建议是请求到3个片之后才开始播放。所以一般很少用HLS做为互联网直播的传输协议。假设列表里面的包含5个 ts 文件，每个 TS 文件包含5秒的视频内容，那么整体的延迟就是25秒。苹果官方推荐的ts时长时10s，所以这样就会大改有30s（n x 10）的延迟。

2.文件碎片化严重：对于点播服务来说, 由于 TS 切片通常较小, 海量碎片在文件分发, 一致性缓存, 存储等方面都有较大挑战.

HLS协议由三部分组成：HTTP+M3U8+TS

HTTP：传输协议

M3U8：索引文件

TS：音视频媒体信息,视频的编码格式为H.264，音频格式为AAC。

#### HLS的工作原理：

<img src="https://upload-images.jianshu.io/upload_images/7114429-0ad79f9241049905.png?imageMogr2/auto-orient/strip|imageView2/2/w/586" alt="img" style="zoom: 67%;" />



1.填入请求m3u8的url，通过http请求。

2.sever返回一个m3u8的播放列表，该列表包含了5段数据的url。

3.客户端解析m3u8播放列表后，按顺序的拿每一段数据的url去获取ts流。

<img src="https://upload-images.jianshu.io/upload_images/7114429-04e35307cd1bf4c0.png?imageMogr2/auto-orient/strip|imageView2/2/w/1046" alt="img" style="zoom:50%;" />



#### HLS如何切片问题

Media encoder将视频源中的视频数据转码到目标编码格式（H264）的视频数据，之后，在stream segmenter模块将视频切片。切片的结果就是index file（m3u8）和ts文件，如上图。





参考资料:

- [Android 音视频开发打怪升级](https://www.jianshu.com/p/1749d2d43ecb)
- https://www.jianshu.com/p/e9a5e1fe2ad8
- https://blog.csdn.net/leixiaohua1020
- [雷神博客](https://www.jianshu.com/p/f5a1c9318524)
- [视频传输协议详解](https://www.jianshu.com/p/c04d810b7562)
- [免费MP4下载](https://wedistill.io/videos)


